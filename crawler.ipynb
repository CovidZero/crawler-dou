{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pandas as pd \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Label\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipywidgets\n",
    "#pip install selenium\n",
    "#pip install spacy\n",
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlynumbers(gimmestring):\n",
    "    num = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    val = \"\"\n",
    "    for letter in gimmestring:\n",
    "        if(letter in num):\n",
    "            val = val+letter\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funções responsáveis por efetuar uma extração mais \"inteligente\" das informações brutas capturadas pelo robô \n",
    "### das páginas dos diários oficiais.\n",
    "\n",
    "## Instala e carrega o idioma português no spacy\n",
    "#!python -m spacy download pt_core_news_sm ## Descomentar na primeira vez que rodar (caso não tenha o pacote já instalado)\n",
    "## Depois que instalar, resetar o kernel e comentar a linha de cima\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "##### FUNÇÕES PARA EXTRAÇÃO DAS INFORMAÇÕES DO CAMPO \"TITLE\" #####\n",
    "\n",
    "\n",
    "## Extração dos campos nomeDiário, data e numeracaoPagina do campo \"Title\"\n",
    "def extract_data_from_title(strTitle):\n",
    "    \n",
    "    ## Transforma o tipo str no tipo doc\n",
    "    docTitle = nlp(u''+strTitle)\n",
    "    \n",
    "    ## Campo nomeDiario\n",
    "    diaryName = strTitle.split('-')[-1]\n",
    "    \n",
    "    ## Campo data\n",
    "    date = extract_date(docTitle)\n",
    "    \n",
    "    ## Campo numeracaoPagina\n",
    "    page = extract_page_number(docTitle)\n",
    "    \n",
    "    return diaryName, date, page\n",
    "    \n",
    "\n",
    "## Extração da data no formato string\n",
    "def extract_date(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    ## Padrão: XX/XX/XXXX\n",
    "    datePattern = [{\"SHAPE\": \"dd/dd/dddd\"},]\n",
    "\n",
    "    ## Procura pelo padrão na entrada nlp_doc\n",
    "    matcher.add('DATE', None, datePattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "\n",
    "    ## Verifica se foi encontrado o padrão de data e retorna a data\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "    else:\n",
    "        ## No caso de não encontrar data, retorna string vazia\n",
    "        return ''\n",
    "\n",
    "## Extração do numero da página no formato string\n",
    "def extract_page_number(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    ## Padrões: Pág. X, Pág. XX, Pág. XXX, Pág. XXXX\n",
    "    pageNumberPattern1 = [{\"LOWER\": \"pág\"}, {\"IS_PUNCT\": True}, {\"SHAPE\": \"d\"}]\n",
    "    pageNumberPattern2 = [{\"LOWER\": \"pág\"}, {\"IS_PUNCT\": True}, {\"SHAPE\": \"dd\"}]\n",
    "    pageNumberPattern3 = [{\"LOWER\": \"pág\"}, {\"IS_PUNCT\": True}, {\"SHAPE\": \"ddd\"}]\n",
    "    pageNumberPattern4 = [{\"LOWER\": \"pág\"}, {\"IS_PUNCT\": True}, {\"SHAPE\": \"dddd\"}]\n",
    "\n",
    "    ## Procura pelos padrões na entrada nlp_doc\n",
    "    matcher.add('PAGE_NUMBER', None, pageNumberPattern1, pageNumberPattern2, \n",
    "                pageNumberPattern3, pageNumberPattern4)\n",
    "    matches = matcher(nlp_doc)\n",
    "\n",
    "    ## Verifica se foi encontrado o padrão de página e extrai apenas o número da página\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        foundPattern = span.text\n",
    "        return foundPattern.split(' ')[1]\n",
    "    else:\n",
    "        ## No caso de não encontrar o padrão, retorna vazio (sem página)\n",
    "        return ''\n",
    "\n",
    "\n",
    "    \n",
    "##### FUNÇÕES PARA EXTRAÇÃO DAS INFORMAÇÕES DO CAMPO \"BODY\" #####\n",
    "\n",
    "    \n",
    "## Extração do decreto no formato string\n",
    "def extract_decree(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    decreePatern1 = [{\"LOWER\": \"art\"}]\n",
    "\n",
    "    matcher.add('DECREE_NUMBER', None, decreePatern1)\n",
    "\n",
    "    matches = matcher(nlp_doc)\n",
    "    occurrenceL = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        occurrenceL.append(span.text)\n",
    "    return occurrenceL\n",
    "\n",
    "\n",
    "## Verifica se há citação de algum decreto em cada linha de um documento.\n",
    "## Se houver, então a linha é totalmente capturada.\n",
    "## Entrada \"conteudo\" representa o campo \"body\"\n",
    "\n",
    "def capture_decree_activities(content):\n",
    "\n",
    "    activities = []\n",
    "\n",
    "    for line in str(content).split('\\nArt.'):\n",
    "        \n",
    "        ## Criação do documento\n",
    "        document = nlp(u''+line)\n",
    "\n",
    "        ## Verifica se algum decreto é citado na linha\n",
    "        result = extract_decree(document)\n",
    "\n",
    "        ## Caso houver citação de um decreto na linha\n",
    "        if 'estabelecimento' in line:\n",
    "            activities.append(line)\n",
    "\n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchcount():\n",
    "    count = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[1]/div[1]/div[1]\")\n",
    "    \n",
    "    return count.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "path = \"chromedriver.exe\"\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta função cria uma lista com as informações dentro da página de busca do datajus\n",
    "\n",
    "# datajus/diarios/\n",
    "\n",
    "# title = titulo do artigo\n",
    "# publisher = publicante do artigo\n",
    "# body = conteúdo da página encontrada com a keyword referenciada\n",
    "# link = link do artigo \n",
    "\n",
    "# diary = nome do diário extraído do título\n",
    "# date = data de publicação do diário extraída do título\n",
    "# page = página do diário extraído do título\n",
    "\n",
    "def page_list():\n",
    "\n",
    "    ## Lista com as informações brutas da página de busca\n",
    "    dataL = []    \n",
    "    \n",
    "    #document_folder é a DIV dentro do HTML que representa todos os documentos dentro  da página\n",
    "    #document_children serão os elementos filhos de 1º grau do elemento document_folder (cada um dos documentos)\n",
    "    \n",
    "    #usaremos isso para criar a condicional de visualização do crawler\n",
    "    document_folder = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]\")\n",
    "    document_children = document_folder.find_elements_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]/*\")\n",
    "    \n",
    "    x = 1\n",
    "    for i in document_children:\n",
    "        #se o atributo \"class\" do elemento filho se chamar 'searchpaginator', signfica que ele é o page_selector\n",
    "        if((i.get_attribute('class')) == 'Pagination pagination SearchPaginator'):\n",
    "            break\n",
    "        publisher = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]/div[{}]/div/div[1]\".format(x))\n",
    "        \n",
    "        title = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]/div[{}]/div/h2\".format(x))\n",
    "        body = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]/div[{}]/div/div[2]\".format(x))\n",
    "        link = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[2]/div[2]/div[{}]/div/h2/a\".format(x))\n",
    "        link_text = link.get_attribute(\"hre\n",
    "        ## Extração dos campos NomeDiario, Data e Página\n",
    "        diary, date, page = extract_data_from_title(title.text)\n",
    "        \n",
    "        dataL.append([diary, date, page, link_text, body.text])\n",
    "        x = x+1\n",
    "    \n",
    "    return dataL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pesquisa todos os filtros de jurisprudencia dentro do site\n",
    "def jurisprudencia():\n",
    "    jurisprudencia_mais = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[1]/div/div/div[3]/label\")\n",
    "    jurisprudencia_mais.click()\n",
    "    \n",
    "    jurisprudencia = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[1]/div/div/div[3]\")\n",
    "    juris_children = jurisprudencia.find_elements_by_class_name('DefaultFilter-item')\n",
    "    \n",
    "    texto = []\n",
    "    value = []\n",
    "    \n",
    "    jurisprudencia = pd.DataFrame()\n",
    "    \n",
    "    texto.append('none')\n",
    "    value.append('')\n",
    "    \n",
    "    for i in juris_children:\n",
    "        texto.append(i.text)\n",
    "        value.append(i.get_attribute('data-value'))\n",
    "        \n",
    "    jurisprudencia[\"descricao\"] = (texto)\n",
    "    jurisprudencia[\"value\"] = (value)\n",
    "    return jurisprudencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URL():\n",
    "    article_type = driver.find_element_by_xpath(\"//*[@id='app-root']/div/div/div[1]/div[1]/div/div/div[4]\")\n",
    "    article_children = article_type.find_elements_by_class_name('DefaultFilter-item')\n",
    "    \n",
    "    texto = []\n",
    "    href = []\n",
    "    \n",
    "    url_list = pd.DataFrame()\n",
    "    \n",
    "    for i in article_children:\n",
    "        texto.append(i.text)\n",
    "        href.append(i.get_attribute('data-artifact'))\n",
    "    url_list[\"descricao\"] = (texto)\n",
    "    url_list[\"url\"] = (href)\n",
    "    \n",
    "    url_list[\"url\"][0] = 'none'\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.jusbrasil.com.br/diarios/busca?q=\"+\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisprudencia = jurisprudencia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = URL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diarios(URL,keyword,other):\n",
    "    df_all = pd.DataFrame()\n",
    "    \n",
    "    pages = 1\n",
    "    result_num = 60\n",
    "    while len(df_all) < result_num:\n",
    "        search = \"busca?q=\"+keyword+\"&p=\"+str(pages)+other\n",
    "        if(pages==1):\n",
    "            driver.get(URL+\"busca?q=\"+keyword+other)\n",
    "            \n",
    "            result_count = searchcount()\n",
    "            result_num = int(onlynumbers(result_count))\n",
    "            print(str(result_num)+\" resultados encontrados\")\n",
    "            \n",
    "            pages_txt = int(result_num/10)+1\n",
    "            print(str(pages_txt)+\" paginas para crawlear\")\n",
    "        else:\n",
    "            driver.get(URL+search)\n",
    "        infoL = page_list()\n",
    "        try:\n",
    "            df_all = df_all.append(infoL)\n",
    "            print(str(len(df_all))+\" resultados encontrados\")\n",
    "        except:\n",
    "            print(\"pagina \"+str(pages)+\" sem resultados\")\n",
    "        \n",
    "        pages += 1\n",
    "        \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9732db1933c4bcea0f36f1b6ee1544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='escolha_url', options=('Tudo', 'Artigos', 'Notícias', 'Jurisprudên…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual\n",
    "def choose(escolha_url=url_list[\"descricao\"],\n",
    "          date_from = widgets.DatePicker(\n",
    "            description=\"from\",\n",
    "            disabled=False),\n",
    "          date_to = widgets.DatePicker(\n",
    "            description=\"to\",\n",
    "            disabled=False),\n",
    "          keyword = widgets.Text(\n",
    "            value=\"covid+fechamento\",\n",
    "            description=\"keywords\"),\n",
    "          juris = widgets.SelectMultiple(\n",
    "            options=jurisprudencia[\"descricao\"],\n",
    "            value=['none'],\n",
    "            disabled=False,\n",
    "            style={'description_width': 'initial'})):\n",
    "    \n",
    "    for i in range(len(url_list)):\n",
    "        if(escolha_url==url_list.iloc[i][\"descricao\"]):\n",
    "            url = (url_list.iloc[i][\"url\"])\n",
    "    \n",
    "    if(date_from and date_to):\n",
    "        other = (\"&dateFrom=\"+str(date_from)+\"&dateTo=\"+str(date_to))\n",
    "    else:\n",
    "        other = ''\n",
    "    \n",
    "    if(url=='none'):\n",
    "        df_all = find_diarios('https://www.jusbrasil.com.br/diarios/','covid+fechamento',other)\n",
    "        df_all = df_all.reset_index()\n",
    "        df_all.to_excel(r'Result.xlsx', )\n",
    "        return df_all\n",
    "    else:\n",
    "        string = 'https://www.jusbrasil.com.br/'+url\n",
    "        df_all = find_diarios(string,'covid+fechamento',other)\n",
    "        df_all = df_all.reset_index()\n",
    "        df_all.to_excel(r'Result.xlsx', )\n",
    "        return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
